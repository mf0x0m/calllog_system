import asyncio
from pathlib import Path
from datetime import datetime, timedelta, date
import jpholiday
import pandas as pd
from playwright.async_api import async_playwright
from service.playwright_utils import login_to_webinsource
from config.settings import settings

CATEGORY_VALUES = ["1", "2"]
DATA_DIR = Path(__file__).resolve().parent.parent / "downloads"
DATA_DIR.mkdir(parents=True, exist_ok=True)

KEEP_COLUMNS = ["No", "é–‹å‚¬æ—¥", "æ™‚é–“", "ç ”ä¿®å", "ä¼šå ´å", "ROOM", "Webé€£æºID",
                "å·¥ç¨‹è¡¨ID", "è¬›å¸«", "ãµã‚ŠãŒãª", "ZoomID", "ZoomPW"]

VENUE_REPLACE_MAP = {
    "ã€æ±äº¬ã€‘ã‚¤ãƒ³ã‚½ãƒ¼ã‚¹å…¬é–‹è¬›åº§ã‚»ãƒŸãƒŠãƒ¼ãƒ«ãƒ¼ãƒ ï¼ˆç¬¬ï¼’é¾åé¤¨ãƒ“ãƒ«ã€€å—ä»˜ï¼•éšï¼‰": "é¾åé¤¨5F",
    "ã€æ±äº¬ã€‘ã‚¤ãƒ³ã‚½ãƒ¼ã‚¹å…¬é–‹è¬›åº§ã‚»ãƒŸãƒŠãƒ¼ãƒ«ãƒ¼ãƒ ï¼ˆç¬¬ï¼’é¾åé¤¨ãƒ“ãƒ«ã€€å—ä»˜ï¼’éšï¼‰": "é¾åé¤¨2F",
    "ã€ç¦å²¡ã€‘ã‚¤ãƒ³ã‚½ãƒ¼ã‚¹ä¹å·ãƒ“ãƒ«ã‚»ãƒŸãƒŠãƒ¼ãƒ«ãƒ¼ãƒ ": "ç¦å²¡",
    "ã€å¤§é˜ªã€‘è‚¥å¾Œæ©‹ã‚»ãƒ³ã‚¿ãƒ¼ãƒ“ãƒ«ï¼ˆå¤§é˜ªæ”¯ç¤¾ã‚»ãƒŸãƒŠãƒ¼ãƒ«ãƒ¼ãƒ ï¼‰": "å¤§é˜ª",
    "ã€åå¤å±‹ã€‘ã‚¤ãƒ³ã‚½ãƒ¼ã‚¹åå¤å±‹ã‚»ãƒŸãƒŠãƒ¼ãƒ«ãƒ¼ãƒ ": "åå¤å±‹",
    "ã€ä»™å°ã€‘ã‚¤ãƒ³ã‚½ãƒ¼ã‚¹æ±åŒ—æ”¯ç¤¾ã‚»ãƒŸãƒŠãƒ¼ãƒ«ãƒ¼ãƒ ": "ä»™å°",
    "ã€æ±äº¬ã€‘ç™½å±±ã‚»ãƒŸãƒŠãƒ¼ãƒ«ãƒ¼ãƒ ï¼ˆã‚¤ãƒ³ã‚½ãƒ¼ã‚¹ç™½å±±ãƒ“ãƒ«ï¼‰": "ç™½å±±",
    "ã€æœ­å¹Œã€‘ã‚¤ãƒ³ã‚½ãƒ¼ã‚¹åŒ—æµ·é“æ”¯ç¤¾ã‚»ãƒŸãƒŠãƒ¼ãƒ«ãƒ¼ãƒ ": "æœ­å¹Œ",
    "ã€åºƒå³¶ã€‘åºƒå³¶YMCAå›½éš›æ–‡åŒ–ã‚»ãƒ³ã‚¿ãƒ¼ã€€3å·é¤¨": "åºƒå³¶",
}

def fmt_date(d: date) -> str:
    return d.strftime("%Y/%m/%d")

def is_business_day(d: date) -> bool:
    return d.weekday() < 5 and not jpholiday.is_holiday(d)

def normalize_room(x):
    try:
        f = float(x)
        return str(int(f)) if f.is_integer() else str(f)
    except:
        return x

#async def download_plants(p, target_dates: list[datetime], login_id: str, plants_password: str):
#    try:
#        print("åˆ¥ã‚µã‚¤ãƒˆã®CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹")
#
#        browser = await p.chromium.launch(headless=True)
#        context = await browser.new_context(accept_downloads=True)
#        page = await context.new_page()
#
#        # â‘  åˆæœŸURLã«ã‚¢ã‚¯ã‚»ã‚¹ï¼ˆãƒ­ã‚°ã‚¤ãƒ³ç”»é¢ã«ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã•ã‚Œã‚‹ï¼‰
#        await page.goto("http://192.168.0.8/plants/t_koukaikouzakanris")
#        await page.wait_for_selector("#LoginuserLoginId", timeout=10000)
#
#        # â‘¡ ãƒ­ã‚°ã‚¤ãƒ³å‡¦ç†
#        await page.fill("#LoginuserLoginId", login_id)
#        await page.fill("#LoginuserPassword", plants_password)
#        await page.click("#btnLogin")
#        print("åˆ¥ã‚µã‚¤ãƒˆãƒ­ã‚°ã‚¤ãƒ³å®Œäº†")
#
#        # â‘¢ å¯¾è±¡ãƒšãƒ¼ã‚¸ã«ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã•ã‚Œã‚‹
#        await page.wait_for_selector("#TKoukaikouzakanriFormExcelKenshuubiShuuryoubi", timeout=10000)
#        print(f"æ¤œç´¢ç”»é¢åˆ°é”")
#
#        # â‘£ çµ‚äº†æ—¥ï¼ˆå¯¾è±¡æœŸé–“ã®æœ«æ—¥ï¼‰ã‚’å…¥åŠ›
#        end_date = fmt_date(target_dates[-1])
#        await page.fill("#TKoukaikouzakanriFormExcelKenshuubiShuuryoubi", end_date)
#        print(f"å…¬é–‹è¬›åº§ çµ‚äº†æ—¥å…¥åŠ›: {end_date}")
#
#        # â‘¤ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†
#        async with page.expect_download() as download_info:
#            await page.click("#Excel")
#        download = await download_info.value
#
#        # ğŸ”½ ãƒ•ã‚¡ã‚¤ãƒ«åãã®ã¾ã¾ã§ä¿å­˜
#        download_path = DATA_DIR / download.suggested_filename
#        await download.save_as(str(download_path))
#        print(f"åˆ¥ã‚µã‚¤ãƒˆCSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {download_path.name}")
#
#    except Exception as e:
#        print(f"[åˆ¥ã‚µã‚¤ãƒˆ] ã‚¨ãƒ©ãƒ¼: {e}")
#    finally:
#        await context.close()
#        await browser.close()


async def set_filters(page, target_date: datetime):
    date_str = fmt_date(target_date)
    await page.wait_for_selector("#open_training_app_cd", state="attached", timeout=10000)
    await page.evaluate(f"""
        const el = document.querySelector("#open_training_app_cd");
        el.value = '';
        [...el.options].forEach(opt => {{
            if ({CATEGORY_VALUES}.includes(opt.value)) opt.selected = true;
        }});
    """)
    await page.fill("#date_from_calender", date_str)
    await page.fill("#date_to_calender", date_str)
    print(f"{date_str} é–‹å‚¬æ—¥ã‚’è¨­å®š")

async def download_webinsource_for_date(p, target_date: datetime, login_id: str, password: str):
    date_str = fmt_date(target_date)
    browser = await p.chromium.launch(headless=True)
    context = await browser.new_context(accept_downloads=True)
    page = await context.new_page()

    try:
        await login_to_webinsource(page, login_id, password)
        print(f"{date_str} ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸ")
        await page.goto(settings.WEBINSOURCE_KENSHU_URL)
        print(f"{date_str} ç ”ä¿®ç®¡ç†ãƒšãƒ¼ã‚¸ã¸é·ç§»")

        await set_filters(page, target_date)
        await page.click("#btnSearch", timeout=180000)
        await page.wait_for_selector("ul.pagination", timeout=180000)
        print(f"{date_str} æ¤œç´¢çµæœèª­ã¿è¾¼ã¿å®Œäº†")

        async with page.expect_download() as download_info:
            await page.click("#btnCsv", timeout=180000, no_wait_after=True)
        download = await download_info.value

        filename = target_date.strftime("%y%m%d") + "_webinsource.csv"
        file_path = DATA_DIR / filename
        await download.save_as(str(file_path))
        print(f"{date_str} CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")

        df = pd.read_csv(file_path, encoding="cp932")

        if "é–‹å‚¬æ—¥" in df.columns:
            df["é–‹å‚¬æ—¥"] = date_str

        if "ä¼šå ´å" in df.columns:
            df["ä¼šå ´å"] = df["ä¼šå ´å"].replace(VENUE_REPLACE_MAP)
            df["ä¼šå ´å"] = df["ä¼šå ´å"].str.replace(
                r"ã€æ±äº¬ã€‘ã‚¤ãƒ³ã‚½ãƒ¼ã‚¹(.+?)ã‚»ãƒŸãƒŠãƒ¼ãƒ«ãƒ¼ãƒ ", r"\1", regex=True
            )

        if "ROOM" in df.columns and "ä¼šè­°å®¤å" in df.columns:
            df["ROOM"] = df["ROOM"].fillna(df["ä¼šè­°å®¤å"])
            df.drop(columns=["ä¼šè­°å®¤å"], inplace=True)

        if "ROOM" in df.columns:
            df["ROOM"] = df["ROOM"].apply(normalize_room)

        # --- å®‰å®šç‰ˆ mapå‡¦ç†ï¼ˆdictå¤‰æ›çµŒç”±ï¼‰ ---
        koukaikouza_files = sorted(DATA_DIR.glob("koukaikouza_*.xlsx"))
        if koukaikouza_files:
            koukaikouza_df = pd.read_excel(koukaikouza_files[-1])
            koukaikouza_df.columns = koukaikouza_df.columns.str.replace("\n", "", regex=False)

            if "Webé€£æºID" in koukaikouza_df.columns:
                koukaikouza_df = koukaikouza_df.dropna(subset=["Webé€£æºID"]).drop_duplicates("Webé€£æºID")

                for col in ["å·¥ç¨‹è¡¨ID", "è¬›å¸«", "ãµã‚ŠãŒãª", "MeetingID", "Meetingãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰"]:
                    if col in koukaikouza_df.columns:
                        mapping_dict = koukaikouza_df.set_index("Webé€£æºID")[col].to_dict()
                        df[col] = df["Webé€£æºID"].map(mapping_dict)

                if "Meetingãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰" in df.columns:
                    df["Meetingãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰"] = df["Meetingãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰"].str.replace("\n", " ", regex=False)

        df.rename(columns={
            "MeetingID": "ZoomID",
            "Meetingãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰": "ZoomPW"
        }, inplace=True)

        df.to_csv(file_path, index=False, encoding="cp932")
        print(f"{date_str} CSVæ•´å½¢ãƒ»ä¿å­˜å®Œäº†")

    except Exception as e:
        print(f"[{date_str}] ã‚¨ãƒ©ãƒ¼: {e}")
    finally:
        await context.close()
        await browser.close()

def merge_csv_and_renumber():
    today_str = datetime.today().strftime("%y%m%d")
    output_file = DATA_DIR / f"{today_str}_webinsource_merged.csv"

    files = sorted(DATA_DIR.glob("*_webinsource.csv"))
    merged = pd.DataFrame()
    for file in files:
        df = pd.read_csv(file, encoding="cp932")
        df = df[[col for col in KEEP_COLUMNS if col in df.columns]]
        merged = pd.concat([merged, df], ignore_index=True)

    if "No" in merged.columns:
        merged["No"] = range(1, len(merged) + 1)

    merged.to_csv(output_file, index=False, encoding="cp932")
    print(f"CSVãƒãƒ¼ã‚¸å®Œäº†: {output_file.name}")

    for file in files:
        if file.name != output_file.name:
            try:
                file.unlink()
            except Exception as e:
                print(f"[å‰Šé™¤å¤±æ•—] {file.name}: {e}")

async def run_parallel_downloads(login_id: str, password: str):
    days_ahead = 10
    concurrency = 4

    today = date.today()
    target_dates = [
        datetime.combine(today + timedelta(days=i), datetime.min.time())
        for i in range(days_ahead + 1)
        if is_business_day(today + timedelta(days=i))
    ]

    sem = asyncio.Semaphore(concurrency)

    async with async_playwright() as p:

        #await download_plants(p, target_dates, login_id, plants_password)

        async def sem_task(d):
            async with sem:
                await download_webinsource_for_date(p, d, login_id, password)

        await asyncio.gather(*(sem_task(d) for d in target_dates))

    merge_csv_and_renumber()

if __name__ == "__main__":
    login_id = "suc1588"
    password = "-7EEuf-eniWxVQ"
#    plants_password = "hD2JJYir!7KjNS"
    asyncio.run(run_parallel_downloads(login_id, password))

#if __name__ == "__main__":
#    import asyncio
#    from datetime import date, timedelta, datetime
#    from playwright.async_api import async_playwright
#
#    login_id = "suc1588"
#    password = "-7EEuf-eniWxVQ"
#    days_ahead = 10
#
#    today = date.today()
#    target_dates = [
#        datetime.combine(today + timedelta(days=i), datetime.min.time())
#        for i in range(days_ahead + 1)
#        if is_business_day(today + timedelta(days=i))
#    ]
#
#    async def test_download_plants():
#        async with async_playwright() as p:
#            await download_plants(p, target_dates, login_id, password)
#
#    asyncio.run(test_download_plants())
